{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data_dict = pickle.load(file, encoding='bytes')\n",
    "    images = data_dict[b'data']\n",
    "    labels = data_dict[b'labels']\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "train_images1, train_labels1 = load_cifar10_batch('./cifar/data_batch_1')\n",
    "train_images2, train_labels2 = load_cifar10_batch('./cifar/data_batch_2')\n",
    "train_images3, train_labels3 = load_cifar10_batch('./cifar/data_batch_3')\n",
    "train_images4, train_labels4 = load_cifar10_batch('./cifar/data_batch_4')\n",
    "train_images5, train_labels5 = load_cifar10_batch('./cifar/data_batch_5')\n",
    "test_images, test_labels = load_cifar10_batch('./cifar/test_batch')\n",
    "\n",
    "train_images1 = train_images1.reshape((10000, 3, 32, 32))\n",
    "train_images2 = train_images2.reshape((10000, 3, 32, 32))\n",
    "train_images3 = train_images3.reshape((10000, 3, 32, 32))\n",
    "train_images4 = train_images4.reshape((10000, 3, 32, 32))\n",
    "train_images5 = train_images5.reshape((10000, 3, 32, 32))\n",
    "test_images = test_images.reshape((10000, 3, 32, 32))\n",
    "\n",
    "train_images = np.concatenate((train_images1, train_images2, train_images3,\n",
    "                               train_images4, train_images5),\n",
    "                              axis=0)\n",
    "# print(train_images.shape)   # (50000, 3, 32, 32)\n",
    "\n",
    "\n",
    "def segment(image, N=2):\n",
    "    '''\n",
    "    Args:\n",
    "        image: ndarray with shape (batch_size*3*32*32)\n",
    "        N: to segment the image to N*N blocks\n",
    "    Return:\n",
    "        blocks_shuffled: batch_size * (N*N) * 3 * (32//N) * (32//N)\n",
    "        labels: batch_size * (N*N) * (N*N)\n",
    "    '''\n",
    "    assert 32 % N == 0, \"The target size is illegal.\"\n",
    "\n",
    "    batch_size, channels, height, width = image.shape\n",
    "    assert channels == 3, \"Number of channels is not 3.\"\n",
    "    assert height == 32, \"Height is not 32.\"\n",
    "    assert width == 32, \"Width is not 32.\"\n",
    "\n",
    "    block_height = 32 // N\n",
    "    block_width = 32 // N\n",
    "    blocks_sorted = image.reshape(batch_size, channels, N, block_height, N,\n",
    "                                  block_width)\n",
    "\n",
    "    blocks_sorted = blocks_sorted.transpose(\n",
    "        0, 2, 4, 1, 3, 5)  # batch_size * N * N * 3 * (32//N) * (32//N)\n",
    "\n",
    "    blocks_sorted = blocks_sorted.reshape(batch_size, N * N, 3, 32 // N,\n",
    "                                          32 // N)\n",
    "\n",
    "    blocks_shuffled = np.zeros((batch_size, N * N, 3, 32 // N, 32 // N),\n",
    "                               dtype=np.uint8)\n",
    "    labels = np.zeros((batch_size, N * N, N * N), dtype=int)\n",
    "    for img_index in range(batch_size):\n",
    "        shuffle_indices = np.random.permutation(N * N)\n",
    "        blocks_shuffled[img_index] = blocks_sorted[img_index, shuffle_indices]\n",
    "        for i, j in enumerate(shuffle_indices):\n",
    "            labels[img_index, i, j] = 1\n",
    "\n",
    "    return blocks_shuffled, labels\n",
    "\n",
    "\n",
    "train_blocks, train_p = segment(train_images, 2)  # 50000 * (N*N) * 3 * (32//N) * (32//N); 50000 * (N*N) * (N*N)\n",
    "test_blocks, test_p = segment(test_images, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jittor as jt\n",
    "from jittor.dataset import VarDataset, DataLoader\n",
    "\n",
    "jt.flags.use_cuda = 1\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_blocks_var = jt.array(train_blocks)\n",
    "train_p_var = jt.array(train_p)\n",
    "train_dataset = VarDataset(train_blocks_var, train_p_var)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_blocks_var = jt.array(test_blocks)\n",
    "test_p_var = jt.array(test_p)\n",
    "test_dataset = VarDataset(test_blocks_var, test_p_var)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jittor as jt\n",
    "from jittor import nn, Module\n",
    "import pygmtools\n",
    "# import cvxpy as cp\n",
    "pygmtools.BACKEND = 'jittor'\n",
    "\n",
    "conv1_output = 64\n",
    "\n",
    "class Model(Module):\n",
    "    # The image blocks are 16 * 16\n",
    "    # Input: batch_size * 4 * 3 * 16 * 16\n",
    "    def __init__(self, sinkhorn_flag=False, dropout_p=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.sinkhorn = sinkhorn_flag\n",
    "\n",
    "        self.conv1 = nn.Conv(3, conv1_output, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm(conv1_output)\n",
    "        self.relu = nn.Relu()\n",
    "\n",
    "        self.conv2 = nn.Conv(conv1_output, conv1_output * 4, 3, stride=1, padding=1)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm(conv1_output * 4)\n",
    "\n",
    "        self.pool = nn.Pool(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv1_output * 4 * 8 * 8, conv1_output * 16)\n",
    "\n",
    "        self.dp1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc2 = nn.Linear(conv1_output * 16 * 4, conv1_output * 16 * 8)\n",
    "\n",
    "        self.dp2 = nn.Dropout(dropout_p)\n",
    "        self.fc3 = nn.Linear(conv1_output * 16 * 8, 16)\n",
    "        \n",
    "    def execute(self, input):\n",
    "        # assert input.shape[1] == 4, \"Not 4 blocks each original image.\"\n",
    "        temp = []\n",
    "        input = input.float()\n",
    "        for i in range(4):\n",
    "            x = input[:, i, :, :, :]\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.pool(x)\n",
    "            x = jt.reshape(x, [x.shape[0], -1])     # batch_size * (conv1_output*2*8*8)\n",
    "\n",
    "            x = self.fc1(x)                         # batch_size * (conv1_output*16)\n",
    "            temp.append(x)\n",
    "        merged = jt.cat(temp, dim=1)                # batch_size * (conv1_output*16*4)\n",
    "        \n",
    "        merged = self.relu(merged)\n",
    "        merged = self.dp1(merged)\n",
    "\n",
    "        merged = self.fc2(merged)\n",
    "        merged = self.relu(merged)\n",
    "        merged = self.dp2(merged)\n",
    "        merged = self.fc3(merged)\n",
    "       \n",
    "    \n",
    "        output = merged.reshape(input.shape[0], 4, 4)\n",
    "        \n",
    "        if self.sinkhorn == True:\n",
    "            output = pygmtools.sinkhorn(output)     # batch_size * 4 * 4\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "sinkhorn_flag = False\n",
    "dropout = 0.5\n",
    "epochs = 30\n",
    "\n",
    "model = Model(sinkhorn_flag=sinkhorn_flag, dropout_p=dropout)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.05\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5\n",
    "optimizer = nn.SGD(model.parameters(), learning_rate, weight_decay=weight_decay)  #, momentum, weight_decay)\n",
    "\n",
    "\n",
    "def train(model, train_loader, loss_function, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "\n",
    "    for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.float()\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "       \n",
    "        optimizer.step(loss)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if batch_index % 100 == 0:\n",
    "            print(f\"Train epoch {epoch+1}, batch {batch_index}\\tLoss: \", \"%.5f\" % loss)\n",
    "    return train_loss\n",
    "\n",
    "def test(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch_index, (inputs, targets) in enumerate(test_loader):\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        compare = (outputs.argmax(dim=2)[0] == targets.argmax(dim=2)[0])\n",
    "        \n",
    "        correct = compare.sum().item()\n",
    "\n",
    "        total_correct += correct\n",
    "        total_samples += batch_size * 4\n",
    "\n",
    "    test_acc = total_correct / total_samples\n",
    "    print(f'Epoch {epoch+1}\\tTest acc is {test_acc}\\n')\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, train_loader, loss_function, optimizer, epoch)\n",
    "    train_loss += loss\n",
    "    acc = test(model, test_loader, epoch)\n",
    "    test_acc.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "if sinkhorn_flag:\n",
    "    plt.savefig(\n",
    "        f\"figures/sh_epoch{epochs}_lr{learning_rate}_dp{dropout}_wd{weight_decay}_batch{batch_size}_{conv1_output}_loss.jpg\",\n",
    "        dpi=800)\n",
    "else:\n",
    "    plt.savefig(\n",
    "        f\"figures/epoch{epochs}_lr{learning_rate}_dp{dropout}_wd{weight_decay}_batch{batch_size}_{conv1_output}_loss.jpg\",\n",
    "        dpi=800)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if sinkhorn_flag:\n",
    "    plt.savefig(\n",
    "        f\"figures/sh_epoch{epochs}_lr{learning_rate}_dp{dropout}_wd{weight_decay}_batch{batch_size}_{conv1_output}_acc.jpg\",\n",
    "        dpi=800)\n",
    "else:\n",
    "    plt.savefig(\n",
    "        f\"figures/epoch{epochs}_lr{learning_rate}_dp{dropout}_wd{weight_decay}_batch{batch_size}_{conv1_output}_acc.jpg\",\n",
    "        dpi=800)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89abc0094a69c300f868ab91b57da2ef53b901cbaf193524c5daf8e63454e9ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
